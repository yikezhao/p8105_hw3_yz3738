Homework 3
================
Yike Zhao

This is my solution to HW3.

### Problem 1

``` r
data("instacart")
```

This dataset contains 1384617 rows and 15 columns. Observations are the
level of items in orders by user. There are user / order variables –
user ID, order ID, order day, and order hour. There are also item
variables – name, aisle, department, and some numeric codes.

How many aisles, and which are most items from?

``` r
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # … with 124 more rows

Make a plot that shows the number of items ordered in each aisle with
more than 10000 items ordered.

``` r
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle,n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw3_yz3738_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

Make a table showing the three most popular items in each of the aisles
“baking ingredients”, “dog food care”, and “packaged vegetables
fruits”.

``` r
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

Make a table showing the mean hour of the day at which Pink Lady Apples
and Coffee Ice Cream are ordered on each day of the week.

``` r
instacart %>%
  filter(product_name %in% c("Pink lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 1 x 8
    ## # Groups:   product_name [1]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8

### Problem 2

Read and tidy the accelerometers data.

``` r
accel_df = 
  read_csv(
   "./Data/accel_data.csv"
  ) %>%
  janitor::clean_names() %>%
  mutate(
    activity_1 = as.double(activity_1),
    day = as.factor(day)
    ) %>% 
  
  pivot_longer(
    activity_1:activity_1440,
    names_to = "minute",
    names_prefix = "activity_",
    values_to = "count"
  ) %>% 

   mutate(
     day_type = case_when(
        day %in% c("Monday","Tuesday","Wednesday","Thursday","Friday") ~ "weekday",
        day %in% c("Saturday","Sunday") ~ "weekend"
    ),
    minute = as.integer(minute)
   )
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

This dataset collects the five weeks of accelerometer data of a 63
year-old male with BMI 25. The dataset contains variables: week,
day\_id, day, minute, count, day\_type. It includes 6 columns and 50400
rows.

Aggregate accross minutes to create a total activity variable for each
day, and create a table showing these totals.

``` r
accel_df %>%
  group_by(week, day) %>% 
  summarize(day_total = sum(count))
```

    ## `summarise()` regrouping output by 'week' (override with `.groups` argument)

    ## # A tibble: 35 x 3
    ## # Groups:   week [5]
    ##     week day       day_total
    ##    <dbl> <fct>         <dbl>
    ##  1     1 Friday      480543.
    ##  2     1 Monday       78828.
    ##  3     1 Saturday    376254 
    ##  4     1 Sunday      631105 
    ##  5     1 Thursday    355924.
    ##  6     1 Tuesday     307094.
    ##  7     1 Wednesday   340115.
    ##  8     2 Friday      568839 
    ##  9     2 Monday      295431 
    ## 10     2 Saturday    607175 
    ## # … with 25 more rows

There isn’t any obvious trend according to the table, yet we can see
that the trends for Tuesdays and Wednesdays are relatively more stable.

Make a single-panel plot that shows the 24-hour activity time courses
for each day and use color to indicate day of the week.

``` r
ggplot(accel_df, aes(x = minute, y = count, color = day)) + 
  geom_smooth(se = FALSE)
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

<img src="p8105_hw3_yz3738_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

According to the plot, we can see that there is a peak in the plot of
Sunday and Friday respectively, while the peak in Sunday appears at the
early half of the day, the peak in Friday appears at the end of the day.
For other days in the week, the activity counts are relatively statble
over the day.

### Problem 3

Read and tidy the NY NOAA data.

``` r
data("ny_noaa")
```

``` r
nynoaa_df =  
  ny_noaa %>% 
  separate(date, c("year", "month", "day")) %>%
  
mutate(
  tmax = as.double(tmax)/10,
  tmin = as.double(tmin)/10,
  prcp = prcp/10
)
```

This dataset contains weather data of New York collected by the National
Oceanic and Atmospheric Association (NOAA).It contains variables: id,
year, month, day, prcp, snow, snwd, tmax, tmin. It includes 9 columns
and 2595176 rows.

``` r
nynoaa_df %>% 
  drop_na(snow) %>% 
  count(snow) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4)
```

    ## # A tibble: 3 x 3
    ##    snow       n  rank
    ##   <int>   <int> <int>
    ## 1     0 2008508     1
    ## 2    13   23095     3
    ## 3    25   31022     2

The most commonly observed snowfall values are 0, 13 and 25 mm

Make a two-panel plot showing the average max temperature in January and
in July in each station across years.

``` r
nynoaa_df %>% 
  filter(month %in% c("01","07")) %>% 
  group_by(id,month,year) %>% 
  summarize(tmax_mean = mean(tmax, na.rm = TRUE)) %>% 
  ggplot(aes(x = year, y = tmax_mean, group = id, color = month)) +
  geom_point() +
  facet_grid(. ~ month) +
  labs(
    title = "Mean Max Daily Temperature in January and July",
    x = "Year",
    y = "Mean Max Daily Temperature",
    caption ="Data from the noaa package"
  ) +
  scale_x_discrete(
    breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010), 
    labels = c("1980", "1985", "1990", "1995", "2000", "2005", "2010"))
```

    ## `summarise()` regrouping output by 'id', 'month' (override with `.groups` argument)

    ## Warning: Removed 5970 rows containing missing values (geom_point).

<img src="p8105_hw3_yz3738_files/figure-gfm/unnamed-chunk-12-1.png" width="90%" />

According to the graph, we can see the both of the maximum temperature
of January and July fluctuated through years with the similar pattern
for about every 5 years. There are outliers in January between 1980 and
1985, in January at around 2005, in January between 1990 and 1995, in
July between 1985 and 1990.

Make a two-panel plot showing tmax vs tmin for the full dataset and the
distribution of snowfall values greater than 0 and less than 100
separately by year.

``` r
plot1_df =
nynoaa_df %>% 
  ggplot(aes(x = tmin, y = tmax)) +
  geom_bin2d() +
  theme(legend.position = "right") +
    labs(
    title = "Max Daily Temperature vs. Min Daily Temperature",
    x = "Min Daily Temperature",
    y = "Max Daily Temperature",
    caption = "Data from the noaa package"
  )
plot2_df =
nynoaa_df %>% 
  mutate(year = as.character(year)) %>% 
  filter(snow > 0 & snow < 100) %>% 
  ggplot(aes(x = year,y = snow)) + 
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5,hjust = 1))+
    labs(
    title = "Snowfall Distribution",
    x = "Time",
    y = "Snowfall",
    caption = "Data from the noaa package
    Only snowfall greater than 0 and less than 100 is included"
  )
plot1_df/plot2_df
```

    ## Warning: Removed 1136276 rows containing non-finite values (stat_bin2d).

<img src="p8105_hw3_yz3738_files/figure-gfm/unnamed-chunk-13-1.png" width="90%" />
